{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZIQ7ULuZ8dF9",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h1>CSCI 4140: Natural Language Processing</h1></center>\n",
    "<center><h1>CSCI/DASC 6040: Computational Analysis of Natural Languages</h1></center>\n",
    "\n",
    "<center><h6>Spring 2023</h6></center>\n",
    "<center><h6>Homework 1 - Tokenization and segmentation</h6></center>\n",
    "<center><h6>Due Sunday, January 29, at 11:59 PM</h6></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2bdvfF8W8dGA",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Part 1: Tokenizer basics - 30 points\n",
    "<a id='part1'></a>\n",
    "## Part 1(a) - 10 points\n",
    "\n",
    "Write a function called `get_words` that takes a string `s` as its only argument. The function should return a list of the words in the same order as they appeared in `s`. Note that in this question a “word” is defined as a “space-separated item”. For example:\n",
    "```\n",
    "get_words('The cat in the hat ate the rat in the vat')\n",
    "\n",
    "['The', 'cat', 'in', 'the', 'hat', 'ate', 'the', 'rat', 'in', 'the', 'vat']\n",
    "```\n",
    "Hint: If you don’t know how to approach this problem, read about [str.split()](https://docs.python.org/3/library/stdtypes.html#str.split)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4qpxKaA8mVzB",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Part 1(b) - 10 points\n",
    "\n",
    "Write a function called `count_words` that takes a list of the words of `s` as its only argument and returns a `collections.Counter` that maps a word to the frequency that it occurred in `s`. Use the output of the `get_words` function as the input to this function.\n",
    "\n",
    "```\n",
    "s = 'The cat in the hat ate the rat in the vat'\n",
    "words = get_words(s)\n",
    "count_words(words)\n",
    "\n",
    "Counter({'the': 3, 'in': 2, 'The': 1, 'cat': 1, 'hat': 1, 'ate': 1, 'rat': 1, 'vat': 1})\n",
    "```\n",
    "Notice that this is somewhat unsatisfying because **the** is counted separately from **The**. To fix this, have your `get_words` function be able to lower-case all of the words before returning them. You won’t want to break any previous code you wrote, though (backwards compatibility is important!), so add a new parameter to `get_words` with a default value:\n",
    "\n",
    "```\n",
    "def get_words(s, do_lower=False)\n",
    "```\n",
    "\n",
    "Now, if `get_words` is called the way we were using it above, nothing will change. But if we call `get_words(s, do_lower=True)` then `get_words` should lowercase the string before getting the words. You can make use of `str.lower` to modify the string. When you’re done, the following should work:\n",
    "\n",
    "```\n",
    "s = 'The cat in the hat ate the rat in the vat'\n",
    "words = get_words(s, do_lower=True)\n",
    "count_words(words)\n",
    "\n",
    "Counter({'the': 4, 'in': 2, 'cat': 1, 'hat': 1, 'ate': 1, 'rat': 1, 'vat': 1})\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Part 1(c) - 10 points\n",
    "\n",
    "Write a function called `words_by_frequency` that takes a list of words as its only required argument. The function should return a list of `(word, count)` tuples sorted by count such that the first item in the list is the most frequent item. Items with the same frequency should be in the same order they appear in the original list of words.\n",
    "\n",
    "`words_by_frequency` should, additionally, take a second parameter `n` that specifies the maximum number of results to return. If `n` is passed, then only the `n` most frequent words should be returned. If `n` is not passed, then all words should be returned in order of frequency.\n",
    "\n",
    "```\n",
    "words_by_frequency(words)\n",
    "\n",
    "[('the', 4), ('in', 2), ('cat', 1), ('hat', 1), ('ate', 1), ('rat', 1), ('vat', 1)]\n",
    "\n",
    "\n",
    "words_by_frequency(words, n=3)\n",
    "\n",
    "[('the', 4), ('in', 2), ('cat', 1)]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "# Part 2: Through the rabbit hole - 50 points\n",
    "\n",
    "Next, you will explore some files from [Project Gutenberg](https://www.gutenberg.org), a library of free eBooks for texts outside of copyright.\n",
    "\n",
    "Some of the Gutenberg texts are all available in the `data/gutenberg/` directory.\n",
    "\n",
    "\n",
    "## Part 2(a) - 10 points\n",
    "<a id='part2a'></a>\n",
    "Let's use the copy of Lewis Carroll’s [“Alice’s Adventures in Wonderland”](https://www.gutenberg.org/ebooks/28885) from **data/gutenberg/carroll-alice.txt**. Use your `words_by_frequency` and `count_words` functions from [Part 1](#part1) to explore the text. For the rest of this exercise, you will always lowercase when getting a list of words. You should find that the five most frequent words in the text are:\n",
    "\n",
    "```\n",
    "the      1603\n",
    "and       766\n",
    "to        706\n",
    "a         614\n",
    "she       518\n",
    "```\n",
    "\n",
    "**Note:** If your numbers were right in the previous part, but don’t match here, it may be because of how you’re calling `split`. Take a look at the documentation for `split` to see if there’s a different way you can call it.\n",
    "\n",
    "**Check-In**\n",
    "\n",
    "1. If your `count_words` function is working correctly, it should report that the word **alice** occurs 221 times. Confirm that you get this result with your code.\n",
    "2. The word **alice** actually appears 398 times in the text, though this is not the answer you got for the previous question. Why? Examine the data to see if you can figure it out before continuing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Add your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qyrxlSPnnDNz",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Part 2(b) - 10 points\n",
    "\n",
    "A spoiler for [2(a)](#part2a): there is a deficiency in how we implemented the `get_words` function. When we are counting words, we probably don’t care whether the word was adjacent to a punctuation mark. For example, the word **hatter** appears in the text 57 times, but if we queried the `count_words` dictionary, we would see it only appeared 24 times. However, it also appeared numerous times adjacent to a punctuation mark, so those instances got counted separately:\n",
    "\n",
    "```\n",
    "word_freq = words_by_frequency(words)\n",
    "for (word, freq) in word_freq:\n",
    "    if 'hatter' in word:\n",
    "        print('{:10} {:3d}'.format(word, freq))\n",
    "\n",
    "hatter      24\n",
    "hatter.     13\n",
    "hatter,     10\n",
    "hatter:      6\n",
    "hatters      1\n",
    "hatter's     1\n",
    "hatter;      1\n",
    "hatter.'     1\n",
    "```\n",
    "\n",
    "Our `get_words` function would be better if it separated punctuation from words. We can accomplish this by using the `re.split` function. Be sure to import `re` to make `re.split()` work. Below is a small example that demonstrates how `str.split` works on a small text and compares it to using `re.split`:\n",
    "\n",
    "```\n",
    "text = '\"Oh no, no,\" said the little Fly, \"to ask me is in vain.\"'\n",
    "text.split()\n",
    "\n",
    "['\"Oh', 'no,', 'no,\"', 'said', 'the', 'little', 'Fly,', '\"to', 'ask', 'me', 'is', 'in', 'vain.\"']\n",
    "\n",
    "re.split(r'(\\W)', text)\n",
    "\n",
    "['', '\"', 'Oh', ' ', 'no', ',', '', ' ', 'no', ',', '', '\"', '', ' ', 'said', ' ', 'the',\n",
    "\n",
    " ' ', 'little', ' ', 'Fly', ',', '', ' ', '', '\"', 'to', ' ', 'ask', ' ', 'me', ' ', 'is',\n",
    "\n",
    " ' ', 'in', ' ', 'vain', '.', '', '\"', '']\n",
    "```\n",
    "\n",
    "Note that this is not exactly what we want, but it is a lot closer. In the resulting list, we find empty strings and spaces, but we have also successfully separated the punctuation from the words.\n",
    "\n",
    "Using the above example as a guide, write and test a function called `tokenize` that takes a string as an input and returns a list of words and punctuation, but not extraneous spaces and empty strings. Like `get_words`, `tokenize` should take an optional argument `do_lower` that determines whether the string should be case normalized before separating the words. You don’t need to modify the `re.split()` line: just remove the empty strings, spaces, and newlines.\n",
    "\n",
    "```\n",
    "tokenize(text, do_lower=True)\n",
    "\n",
    "['\"', 'oh', 'no', ',', 'no', ',', '\"', 'said', 'the', 'little', 'fly', ',', '\"', 'to', 'ask', 'me', 'is', 'in', 'vain', '.', '\"']\n",
    "\n",
    "print(' '.join(tokenize(text, do_lower=True)))\n",
    "\n",
    "\" oh no , no , \" said the little fly , \" to ask me is in vain . \"\n",
    "```\n",
    "\n",
    "**Checking In**\n",
    "\n",
    "Use your `tokenize` function in conjunction with your `count_words` function to list the top 5 most frequent words in **carroll-alice.txt**. You should get this:\n",
    "\n",
    "```\n",
    "'        2871      <-- single quote\n",
    ",        2418      <-- comma\n",
    "the      1642\n",
    ".         988      <-- period\n",
    "and       872\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Part 2(c) - 10 points\n",
    "\n",
    "Write a function called `filter_nonwords` that takes a list of strings as input and returns a new list of strings that excludes anything that isn’t entirely alphabetic. Use the `str.isalpha()` method to determine is a string is comprised of only alphabetic characters.\n",
    "\n",
    "```\n",
    "text = '\"Oh no, no,\" said the little Fly, \"to ask me is in vain.\"'\n",
    "tokens = tokenize(text, do_lower=True)\n",
    "filter_nonwords(tokens)\n",
    "\n",
    "['oh', 'no', 'no', 'said', 'the', 'little', 'fly', 'to', 'ask', 'me', 'is', 'in', 'vain']\n",
    "```\n",
    "\n",
    "Use this function to list the top 5 most frequent words in **carroll-alice.txt**. Confirm that you get the following before moving on:\n",
    "\n",
    "```\n",
    "the      1642\n",
    "and       872\n",
    "to        729\n",
    "a         632\n",
    "it        595\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Part 2(d) - 20 points\n",
    "\n",
    "Iterate through all of the files in the **gutenberg** data directory and print out the top 5 words for each. To get a list of all the files in a directory, use the `os.listdir` function:\n",
    "\n",
    "```\n",
    "import os\n",
    "\n",
    "directory = 'data/gutenberg/'\n",
    "files = os.listdir(directory)\n",
    "infile = open(os.path.join(directory, files[0]), 'r', encoding='latin1')\n",
    "```\n",
    "\n",
    "This example also uses the function `os.path.join` that you might want to read about.\n",
    "\n",
    "*Note about encodings:* This `open` function above uses the optional encoding argument to tell Python that the source file is encoded as latin1. Be sure to use this encoding flag to read the files in the **Gutenberg** corpus, as the default (Unicode) won't work!\n",
    "\n",
    "**Token Analysis Questions**\n",
    "\n",
    "Answer the following questions.\n",
    "\n",
    "1. **Most Frequent Word:** Loop through all the files in the **gutenberg** data directory that end in **.txt**. Is **the** always the most common word? If not, what are some other words that show up as the most frequent word (and in which documents)? What do you notice about these words?\n",
    "2. **Impact of Lowercasing:** If you don’t lowercase all the words before you count them, how does this result change, if at all? Discuss what you observe.\n",
    "\n",
    "<font color='red'>Note: If a question (like the one above) asks you to discuss results, that always means both what the results were and what that implies about the world (i.e., your corpus, your method, etc.). A good answer on this sort of question is a paragraph that goes something like \"the result was X, specific interesting examples were X' and X\", this is/isn't surprising because it would imply P or Q, this implies it might be better to do Y / to evaluate Z to learn more\".</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Pu7nnpwBnT6O",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<font color='red'>Your answers go here.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x0DCb05znZIS",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Part 3: Sentence segmentation - 30 points\n",
    "\n",
    "Next, you will write a simple sentence segmenter.\n",
    "\n",
    "The **data/brown** directory includes three English-language text files taken from the Brown Corpus:\n",
    "\n",
    "- `editorial.txt`\n",
    "- `fiction.txt`\n",
    "- `lore.txt`\n",
    "\n",
    "These files represent large strings of natural language text, with no line breaks nor other special symbols to annotate where sentence splits occur. In the data set you are working with, sentences can only end with one of 5 characters: period, colon, semi-colon, exclamation point and question mark.\n",
    "\n",
    "However, there is a catch: not every period represents the end of a sentence. Many abbreviations (U.S.A., Dr., Mon., etc., etc.) that can appear in the middle of a sentence, and the period does not indicate the end of the sentence. (If you have a phone that uses autocomplete to type, you may already have had annoying experiences where it automatically capitalized words after these abbreviations!) These texts also have many examples where colon is not the end of the sentence. The other three punctuation marks are all nearly unambiguously the ends of a sentence (yes, even semi-colons).\n",
    "\n",
    "For each of the above files, I have also provided a file in the same directory containing the **character index** (counting from 0 for the first character) of each of the actual locations of the ends of sentences:\n",
    "\n",
    "- `editorial-eos.txt`\n",
    "- `fiction-eos.txt`\n",
    "- `lore-eos.txt`\n",
    "\n",
    "Your job is to write a sentence segmenter, and to output the predicted token number of each sentence boundary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_1aMANvf0IqF",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Part 3(a) - 10 points\n",
    "\n",
    "Below is some starter code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wQm_L9HT0LA9",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def my_best_segmenter(token_list): \n",
    "    \"\"\" TODO: Replace this with an improved sentence segmenter. \"\"\"\n",
    "    pass\n",
    "\n",
    "def baseline_segmenter(token_list):\n",
    "    all_sentences = []\n",
    "    this_sentence = []\n",
    "    for token in token_list:\n",
    "        this_sentence.append(token)\n",
    "        if token in ['.', ':', ';', '!', '?']:\n",
    "            all_sentences.append(this_sentence)\n",
    "            this_sentence = []\n",
    "    return all_sentences\n",
    "\n",
    "def write_sentence_boundaries(sentence_list, out):\n",
    "    \"\"\" TODO: Write out the token numbers of the sentence boundaries. \"\"\"\n",
    "    pass\n",
    "\n",
    "\"\"\" TODO: Write out the code to parse a text file. \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_zQxW6uWox-S",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "**Checking In**\n",
    "\n",
    "Confirm that your code can open the file **data/brown/editorial.txt** and that your code from the previous part splits it into 63,333 tokens.\n",
    "\n",
    "Note: Do not filter out punctuation, since those tokens will be exactly the ones we want to consider as potential sentence boundaries!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Part 3(b) - 10 points\n",
    "\n",
    "The starter code contains a function called `baseline_segmenter` that takes a list of tokens as its only argument. It returns a list of tokenized sentences; that is, a list of lists of words, with one list per sentence.\n",
    "\n",
    "```\n",
    "baseline_segmenter(tokenize('I am Sam. Sam I am.')\n",
    "\n",
    "[['I', 'am', 'Sam', '.'], ['Sam', 'I', 'am', '.']]\n",
    "```\n",
    "\n",
    "Remember that every sentence in our data set ends with one of the five tokens \\['.', ':', ';', '!', '?'\\]. Since it’s a baseline approach, `baseline_segmenter` predicts that every instance of one of these characters is the end of a sentence.\n",
    "\n",
    "Fill in the function `write_sentence_boundaries`. This function takes two arguments: a list of lists of strings (like the one returned by `baseline_segmenter`) and a pointer to a stream to write output (an open write-enabled file). You will need to loop through all of the sentences in the document. For each sentence, you will want to write the index of the last word in the sentence to the filepointer. Remember that Python lists are 0-indexed!\n",
    "\n",
    "Confirm that when you run `baseline_segmenter` on the file **data/brown/editorial.txt**, it predicts 3278 sentence boundaries, and that the first five predicted boundaries are at tokens 22, 54, 74, 99, and 131."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rGAtIVFhoGJn",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Part 3(c) - extra credit, 10 points\n",
    "\n",
    "Now it’s time to improve the baseline sentence segmenter. We don’t have any false negatives (since we’re predicting that every instance of the possibly-end-of-sentence punctuation marks is, in fact, the end of a sentence), but we have quite a few false positives.\n",
    "\n",
    "There’s a placeholder for a second segmentation function defined in the starter code. You will fill in that `my_best_segmenter` function to do a (hopefully!) better job identifying sentence boundaries. The specifics of how you do so are up to you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UurBjd6m8dGE",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Questions\n",
    "\n",
    "1. Describe the performance of your final segmenter, by comparing some of the sentences it correctly tokenized/generated whereas the `baseline_segmenter` did not.\n",
    "2. Describe at least 3 things that your final segmenter does better than the baseline segmenter and discuss them. What cases are you most proud of catching in your segmenter? Include specific examples that are handled well.\n",
    "3. Describe at least 3 places where your segmenter still makes mistakes and discuss them. Include specific examples where your segmenter makes the wrong decision. If you had another week to work on this, what would you add? If you had the rest of the semester to work on it, what would you do?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Pu7nnpwBnT6O",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<font color='red'>Your answers go here.</font>"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "colab": {
   "collapsed_sections": [],
   "name": "02. Python 1.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
